# Glamira Crawler Project

## ğŸ“ Project Structure

```
glamira_crawler/
â”œâ”€â”€ scripts/                    # Python scripts
â”‚   â”œâ”€â”€ crawl_product_name.py   # Main crawler with threading
â”‚   â”œâ”€â”€ process_ip_location.py  # IP geolocation processing
â”‚   â”œâ”€â”€ test_proxy.py          # Proxy testing utilities
â”‚   â””â”€â”€ test_url.py            # URL testing utilities
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ config.ini             # Main configuration
â”œâ”€â”€ data/                      # Input data and working files
â”‚   â”œâ”€â”€ IP-COUNTRY-REGION-CITY.BIN
â”‚   â”œâ”€â”€ unique_ips.json
â”‚   â”œâ”€â”€ unique_product_ids.json
â”‚   â””â”€â”€ processed_product_ids.json
â”œâ”€â”€ logs/                      # Log files
â”‚   â”œâ”€â”€ product_processing.log
â”‚   â””â”€â”€ product_processing.error.log
â””â”€â”€ output/                    # Output files
   â”œâ”€â”€ product_names.csv     # Successful crawls
   â””â”€â”€ failed_products.csv   # Failed crawls
```

## ğŸš€ Quick Start

### 1. Setup Environment
```bash
# Activate virtual environment
source ../venv/bin/activate

# Install dependencies (if needed)
pip install pymongo requests beautifulsoup4 configparser
```

### 2. Run Crawler
```bash
cd scripts/
python crawl_product_name.py
```

### 3. Check Results
```bash
# View successful crawls
head -20 output/product_names.csv

# View failed crawls
head -20 output/failed_products.csv

# Monitor logs
tail -f logs/product_processing.log
```

## âš™ï¸ Configuration

Edit `config/config.ini`:

- **Threading**: `max_workers = 8` (adjust based on your system)
- **Delays**: `crawl_delay_min_seconds = 0.2` (avoid being blocked)
- **MongoDB**: Update connection string if needed

## ğŸ“Š Features

- âœ… **Multi-threaded crawling** for high performance
- âœ… **Checkpoint saves** every 100 records
- âœ… **Resume capability** after interruption
- âœ… **Thread-safe data handling**
- âœ… **Detailed logging** with error tracking
- âœ… **Failed record tracking** with immediate saves

## ğŸ” Monitoring

- **Real-time progress**: Check `logs/product_processing.log`
- **Error tracking**: Check `logs/product_processing.error.log`
- **Intermediate results**: `output/product_names.csv` updates every 100 records

## ğŸ›¡ï¸ Safety Features

- **Rate limiting**: Configurable delays between requests
- **Error handling**: Graceful handling of HTTP errors
- **Data persistence**: No data loss on crashes
- **Resume support**: Continue from where you left off

## ğŸ“ˆ Performance

- **Speed**: 5-10x faster than single-threaded
- **Typical rate**: ~100-200 URLs per minute (depending on settings)
- **Memory efficient**: Checkpoint saves prevent memory buildup
